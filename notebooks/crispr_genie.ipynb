{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRISPRGenie\n",
    "\n",
    "This is the main notebook for the project *CRISPRGenie*. The current aim of the project is as follows:\n",
    "1. The input dataset contains **sgRNA** sequences for a total of 19k different genes from the human genome. \n",
    "2. The model will be trained on all the sequences to predict different possible sgRNA sequences based on the input target gene.\n",
    "3. The model will be autoregressive, i.e., it will predict new sequences based on its training data. Furthermore, the model will ideally contain a semi-supervised regression task which will allow the model to predict the metrics of the newly predicted sgRNA sequence/s for the given target.\n",
    "4. The metrics include the log2-fold changes and the effect (ranging from -9 to 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model training will take place in two steps:\n",
    "\n",
    "#### Part 1: Generating sgRNA Sequences\n",
    "\n",
    "**Model Design:**\n",
    "\n",
    "* Input: Gene symbol (e.g., ENSG00000148584)\n",
    "* Output: Set of sgRNA sequences\n",
    "\n",
    "**Approach:**\n",
    "\n",
    "* Data Preparation: For training, map each gene symbol to its corresponding sgRNA sequences. This could involve aggregating all sgRNA sequences that target a specific gene into a single training example.\n",
    "* Model Type: Use a generative model like GPT, which is adept at producing sequences. Train the model to generate sgRNA sequences when provided with a gene symbol.\n",
    "\n",
    "**Training:**\n",
    "\n",
    "* Input: Gene symbol.\n",
    "* Output: A sequence of sgRNAs or a concatenated string of multiple sgRNAs.\n",
    "Train the model to maximize the likelihood of generating correct sgRNA sequences given a gene symbol.\n",
    "\n",
    "#### Part 2: Predicting Effect Metrics\n",
    "\n",
    "**Model Design:**\n",
    "\n",
    "* Input: sgRNA sequence\n",
    "* Output: Effect metrics (quantized effect as an integer from -9 to 9)\n",
    "\n",
    "**Approach:**\n",
    "\n",
    "* Data Preparation: Use sgRNA sequences and their corresponding effect metrics from your dataset.\n",
    "* Model Type: A classification model (like BERT used for classification tasks) that can predict a class (effect metric) for each sgRNA sequence.\n",
    "\n",
    "**Training:**\n",
    "\n",
    "* Input: sgRNA sequence.\n",
    "* Output: Effect class.\n",
    "This model can be trained using a cross-entropy loss where each class corresponds to a different quantile of sgRNA efficacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-1 Generating sgRNA Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>chr</th>\n",
       "      <th>strand</th>\n",
       "      <th>pubmed</th>\n",
       "      <th>cellline</th>\n",
       "      <th>condition</th>\n",
       "      <th>sequence</th>\n",
       "      <th>symbol</th>\n",
       "      <th>ensg</th>\n",
       "      <th>log2fc</th>\n",
       "      <th>rc_initial</th>\n",
       "      <th>rc_final</th>\n",
       "      <th>effect</th>\n",
       "      <th>cas</th>\n",
       "      <th>screentype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50844073</td>\n",
       "      <td>50844096</td>\n",
       "      <td>10</td>\n",
       "      <td>+</td>\n",
       "      <td>26472758</td>\n",
       "      <td>Jiyoye</td>\n",
       "      <td>viability</td>\n",
       "      <td>GCAGCATCCCAACCAGGTGGAGG</td>\n",
       "      <td>A1CF</td>\n",
       "      <td>ENSG00000148584</td>\n",
       "      <td>0.315907</td>\n",
       "      <td>{260}</td>\n",
       "      <td>{244}</td>\n",
       "      <td>2</td>\n",
       "      <td>hSpCas9</td>\n",
       "      <td>negative selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50814011</td>\n",
       "      <td>50814034</td>\n",
       "      <td>10</td>\n",
       "      <td>-</td>\n",
       "      <td>26472758</td>\n",
       "      <td>Jiyoye</td>\n",
       "      <td>viability</td>\n",
       "      <td>GCGGGAGTGAGAGGACTGGGCGG</td>\n",
       "      <td>A1CF</td>\n",
       "      <td>ENSG00000148584</td>\n",
       "      <td>2.144141</td>\n",
       "      <td>{17}</td>\n",
       "      <td>{59}</td>\n",
       "      <td>9</td>\n",
       "      <td>hSpCas9</td>\n",
       "      <td>negative selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50836111</td>\n",
       "      <td>50836134</td>\n",
       "      <td>10</td>\n",
       "      <td>+</td>\n",
       "      <td>26472758</td>\n",
       "      <td>Jiyoye</td>\n",
       "      <td>viability</td>\n",
       "      <td>ATGACTCTCATACTCCACGAAGG</td>\n",
       "      <td>A1CF</td>\n",
       "      <td>ENSG00000148584</td>\n",
       "      <td>1.426034</td>\n",
       "      <td>{75}</td>\n",
       "      <td>{153}</td>\n",
       "      <td>8</td>\n",
       "      <td>hSpCas9</td>\n",
       "      <td>negative selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50836095</td>\n",
       "      <td>50836118</td>\n",
       "      <td>10</td>\n",
       "      <td>-</td>\n",
       "      <td>26472758</td>\n",
       "      <td>Jiyoye</td>\n",
       "      <td>viability</td>\n",
       "      <td>GAGTCATCGAGCAGCTGCCATGG</td>\n",
       "      <td>A1CF</td>\n",
       "      <td>ENSG00000148584</td>\n",
       "      <td>1.550133</td>\n",
       "      <td>{47}</td>\n",
       "      <td>{105}</td>\n",
       "      <td>8</td>\n",
       "      <td>hSpCas9</td>\n",
       "      <td>negative selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50816234</td>\n",
       "      <td>50816257</td>\n",
       "      <td>10</td>\n",
       "      <td>-</td>\n",
       "      <td>26472758</td>\n",
       "      <td>Jiyoye</td>\n",
       "      <td>viability</td>\n",
       "      <td>AGTCACCCTAGCAAAACCAGTGG</td>\n",
       "      <td>A1CF</td>\n",
       "      <td>ENSG00000148584</td>\n",
       "      <td>0.382513</td>\n",
       "      <td>{58}</td>\n",
       "      <td>{57}</td>\n",
       "      <td>3</td>\n",
       "      <td>hSpCas9</td>\n",
       "      <td>negative selection</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      start       end chr strand    pubmed cellline  condition  \\\n",
       "0  50844073  50844096  10      +  26472758   Jiyoye  viability   \n",
       "1  50814011  50814034  10      -  26472758   Jiyoye  viability   \n",
       "2  50836111  50836134  10      +  26472758   Jiyoye  viability   \n",
       "3  50836095  50836118  10      -  26472758   Jiyoye  viability   \n",
       "4  50816234  50816257  10      -  26472758   Jiyoye  viability   \n",
       "\n",
       "                  sequence symbol             ensg    log2fc rc_initial  \\\n",
       "0  GCAGCATCCCAACCAGGTGGAGG   A1CF  ENSG00000148584  0.315907      {260}   \n",
       "1  GCGGGAGTGAGAGGACTGGGCGG   A1CF  ENSG00000148584  2.144141       {17}   \n",
       "2  ATGACTCTCATACTCCACGAAGG   A1CF  ENSG00000148584  1.426034       {75}   \n",
       "3  GAGTCATCGAGCAGCTGCCATGG   A1CF  ENSG00000148584  1.550133       {47}   \n",
       "4  AGTCACCCTAGCAAAACCAGTGG   A1CF  ENSG00000148584  0.382513       {58}   \n",
       "\n",
       "  rc_final  effect      cas          screentype  \n",
       "0    {244}       2  hSpCas9  negative selection  \n",
       "1     {59}       9  hSpCas9  negative selection  \n",
       "2    {153}       8  hSpCas9  negative selection  \n",
       "3    {105}       8  hSpCas9  negative selection  \n",
       "4     {57}       3  hSpCas9  negative selection  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/GenomeCRISPR.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ensg</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENSG00000148584</td>\n",
       "      <td>GCAGCATCCCAACCAGGTGGAGG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENSG00000148584</td>\n",
       "      <td>GCGGGAGTGAGAGGACTGGGCGG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENSG00000148584</td>\n",
       "      <td>ATGACTCTCATACTCCACGAAGG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENSG00000148584</td>\n",
       "      <td>GAGTCATCGAGCAGCTGCCATGG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENSG00000148584</td>\n",
       "      <td>AGTCACCCTAGCAAAACCAGTGG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ensg                 sequence\n",
       "0  ENSG00000148584  GCAGCATCCCAACCAGGTGGAGG\n",
       "1  ENSG00000148584  GCGGGAGTGAGAGGACTGGGCGG\n",
       "2  ENSG00000148584  ATGACTCTCATACTCCACGAAGG\n",
       "3  ENSG00000148584  GAGTCATCGAGCAGCTGCCATGG\n",
       "4  ENSG00000148584  AGTCACCCTAGCAAAACCAGTGG"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract only the necessary columns\n",
    "data_relevant = df[['ensg', 'sequence']]\n",
    "\n",
    "# Drop any rows with missing values in these columns to ensure data integrity\n",
    "data_relevant = data_relevant.dropna()\n",
    "\n",
    "# take the first 1k instances as a test\n",
    "data_relevant = data_relevant[:1000]\n",
    "\n",
    "data_relevant.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train, val and test sets\n",
    "train_df, temp_df = train_test_split(data_relevant, test_size=0.7, random_state=42)  # 70% train, 40% temp\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)  # Split temp into 15% val, 15% test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization \n",
    "\n",
    "Since the current dataset has a much smaller vocabulary, I will be going with a custom tokenizer which will be lightweight compared to the pretrained tokenizer of GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    def __init__(self):\n",
    "        # Include all nucleotide bases, special tokens, and necessary characters for gene IDs\n",
    "        self.token_to_id = {\n",
    "            '[PAD]': 0, '[ID]': 1, '[SOS]': 2, '[EOS]': 3,\n",
    "            'A': 4, 'T': 5, 'G': 6, 'C': 7, \n",
    "            'ENSG': 8, \n",
    "            '0': 9, '1': 10, '2': 11, '3': 12, '4': 13,\n",
    "            '5': 14, '6': 15, '7': 16, '8': 17, '9': 18\n",
    "        }\n",
    "        self.id_to_token = {v: k for k, v in self.token_to_id.items()}\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.token_to_id)\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\" Convert text to a list of token IDs, treating each character as a token unless enclosed in []. \"\"\"\n",
    "        tokens = []\n",
    "        i = 0\n",
    "        while i < len(text):\n",
    "            if text[i] == '[':  # Start of a special token\n",
    "                special_token_end = text.find(']', i)\n",
    "                if special_token_end != -1:\n",
    "                    tokens.append(text[i:special_token_end+1])\n",
    "                    i = special_token_end + 1\n",
    "                else:\n",
    "                    tokens.append(text[i])  # Fallback if ']' is missing\n",
    "                    i += 1\n",
    "            elif text[i] == 'E':\n",
    "                id_token = text[i: i+4]\n",
    "                tokens.append(id_token)\n",
    "                i+=4\n",
    "            else:\n",
    "                tokens.append(text[i])\n",
    "                i += 1\n",
    "        return [self.token_to_id[token] for token in tokens if token in self.token_to_id]\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        \"\"\" Convert a list of token IDs back to a string. \"\"\"\n",
    "        return ''.join(self.id_to_token.get(token_id, '') for token_id in token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [1, 8, 9, 9, 9, 9, 9, 10, 13, 17, 14, 17, 13, 2, 6, 7, 4, 6, 7, 4, 5, 7, 7, 7, 4, 4, 7, 7, 4, 6, 6, 5, 6, 6, 4, 6, 6, 3]\n",
      "Decoded: [ID]ENSG00000148584[SOS]GCAGCATCCCAACCAGGTGGAGG[EOS]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = CustomTokenizer()\n",
    "# Example encoding\n",
    "encoded = tokenizer.encode(\"[ID]ENSG00000148584[SOS]GCAGCATCCCAACCAGGTGGAGG[EOS]\")\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(\"Encoded:\", encoded)\n",
    "print(\"Decoded:\", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneSequenceDataset(Dataset):\n",
    "    def __init__(self, gene_ids, sequences, tokenizer):\n",
    "        self.gene_ids = gene_ids\n",
    "        self.sequences = sequences\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.gene_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Form the full sequence with special tokens\n",
    "        full_sequence = f\"[ID]{self.gene_ids[idx]}[SOS]{self.sequences[idx]}[EOS]\"\n",
    "        tokenized_sequence = self.tokenizer.encode(full_sequence)\n",
    "        return torch.tensor(tokenized_sequence, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch_padded = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0)\n",
    "    return batch_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  8,  9,  9,  9,  9,  9, 10, 13, 17, 14, 17, 13,  2,  6,  7,  4,  6,\n",
      "          7,  4,  5,  7,  7,  7,  4,  4,  7,  7,  4,  6,  6,  5,  6,  6,  4,  6,\n",
      "          6,  3],\n",
      "        [ 1,  8,  9,  9,  9,  9,  9, 10, 14, 14, 15, 14, 16,  2,  5,  5,  6,  7,\n",
      "          7,  6,  5,  7,  4,  6,  7,  5,  5,  6,  6,  6,  4,  6,  6,  3,  0,  0,\n",
      "          0,  0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage:\n",
    "gene_ids = ['ENSG00000148584', 'ENSG00000155657']\n",
    "sequences = ['GCAGCATCCCAACCAGGTGGAGG', 'TTGCCGTCAGCTTGGGAGG']\n",
    "tokenizer = CustomTokenizer()  # Make sure your tokenizer is properly defined\n",
    "\n",
    "dataset = GeneSequenceDataset(gene_ids, sequences, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=20, collate_fn=collate_fn)\n",
    "\n",
    "# Quick test to see a batch from DataLoader\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom GPT Model\n",
    "\n",
    "In this section I will be creating a custom GPT model. The architecture and parameters used in this model will be kept similar to the GPT2 model from OpenAI. I will be training this GPT model on the sgRNA dataset and will be comparing the performance of the custom model against the pretrained GPT2 model from HuggingFace Transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import GPT2LMHeadModel\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a causal self-attention mechanism which is a fundamental component of transformer models\n",
    "    designed for sequence processing tasks where the model should not have future insight. This module \n",
    "    ensures that the predictions for a particular position are dependent only on the known outputs at \n",
    "    previous positions.\n",
    "\n",
    "    Attributes:\n",
    "        c_attn (nn.Linear): Linear layer that projects input embeddings into queries, keys, and values.\n",
    "        c_proj (nn.Linear): Linear layer that projects the output of the attention mechanism back to\n",
    "                            the dimension of embeddings.\n",
    "        bias (torch.Tensor): Buffer that applies a triangular mask to ensure attention is only applied\n",
    "                             to preceding positions, preserving causality.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initializes the CausalSelfAttention layer with specific configuration.\n",
    "\n",
    "        Args:\n",
    "            config: A configuration object containing attributes like `n_embd` (embedding size),\n",
    "                    `n_head` (number of attention heads), and `block_size` (sequence length).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Ensuring the embedding size is divisible by the number of heads for even split.\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        # Linear transformation that outputs triple the embedding dimension to split into\n",
    "        # queries, keys, and values.\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "\n",
    "        # Linear transformation for the output of the attention computation.\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "\n",
    "        # Store the number of attention heads and the embedding dimension per head.\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "        # Register a buffer for the triangular mask that prevents attending to future positions.\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                         .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the causal self-attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, sequence_length, embedding_dim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after processing with causal self-attention.\n",
    "        \"\"\"\n",
    "        # Unpack the dimensions of the input tensor.\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # Pass the input through the attention projection layer to get combined query, key, value tensors.\n",
    "        qkv = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "\n",
    "        # Split and reshape the combined QKV tensor into individual Q, K, V tensors and transpose\n",
    "        # for multi-head attention computation.\n",
    "        q, k, v = [tensor.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) for tensor in qkv]\n",
    "\n",
    "        # Compute the attention scores, apply scaling for stability, and use the mask to enforce causality.\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "\n",
    "        # Apply softmax to convert scores to probabilities and compute the weighted sum of values.\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = (att @ v).transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        # Project the output back to the embedding dimension and return.\n",
    "        return self.c_proj(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A multilayer perceptron (MLP) module used within transformer blocks as a position-wise\n",
    "    feed-forward network. This module is a simple neural network for transforming the \n",
    "    representation at every position independently in the sequence.\n",
    "\n",
    "    Attributes:\n",
    "        c_fc (nn.Linear): The first linear layer that expands the input dimension.\n",
    "        gelu (nn.GELU): Gaussian Error Linear Unit (GELU) activation function, which\n",
    "                        allows the model to include non-linearity and helps in learning\n",
    "                        more complex patterns. This version uses the 'tanh' approximation\n",
    "                        for faster computation.\n",
    "        c_proj (nn.Linear): The second linear layer that projects the output back to \n",
    "                            the original embedding dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initializes the MLP module with specified configurations.\n",
    "\n",
    "        Args:\n",
    "            config: A configuration object containing `n_embd`, the size of the input\n",
    "                    and output embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # First linear layer that increases dimensionality 4x to allow more complex interactions.\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        \n",
    "        # GELU activation function with 'tanh' approximation.\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        \n",
    "        # Second linear layer that reduces dimensionality back to the original size.\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the MLP module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor to the MLP with shape (batch_size, sequence_length, n_embd).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after processing through two linear layers\n",
    "                          and a GELU activation function, with the same shape as input.\n",
    "        \"\"\"\n",
    "        # Pass the input through the first linear layer and then apply the GELU activation function.\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        \n",
    "        # Finally, pass the activated output through the second linear layer to match the original embedding size.\n",
    "        x = self.c_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Represents a single Transformer block, which is a fundamental component of the Transformer architecture.\n",
    "    Each block sequentially applies layer normalization, a causal self-attention mechanism, another layer normalization,\n",
    "    and a multilayer perceptron (MLP). The architecture follows a typical pattern used in JXT models,\n",
    "    implementing a residual connection around each of the two main sub-layers (self-attention and MLP).\n",
    "\n",
    "    Attributes:\n",
    "        ln_1 (nn.LayerNorm): Layer normalization applied before the self-attention mechanism.\n",
    "        attn (CausalSelfAttention): The causal self-attention module, ensuring that the predictions\n",
    "                                    for a position are dependent only on the known outputs at previous positions.\n",
    "        ln_2 (nn.LayerNorm): Layer normalization applied before the MLP.\n",
    "        mlp (MLP): The multilayer perceptron module that processes the output of the attention mechanism.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initializes the Transformer block with specified configurations.\n",
    "\n",
    "        Args:\n",
    "            config: A configuration object containing necessary parameters like `n_embd`, which is used\n",
    "                    to set the dimensionality of the layer normalization and to configure the attention and MLP modules.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Layer normalization that normalizes the embeddings before the self-attention layer.\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        \n",
    "        # The self-attention mechanism defined in the CausalSelfAttention class.\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        \n",
    "        # Layer normalization that normalizes the output of the attention mechanism before passing it to the MLP.\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        \n",
    "        # The MLP that further processes the output from the attention mechanism.\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass through the Transformer block.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor to the block with shape (batch_size, sequence_length, n_embd).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor from the block, which has the same shape as the input.\n",
    "                          This output can be fed into subsequent blocks in a Transformer model.\n",
    "        \"\"\"\n",
    "        # Apply layer normalization, then self-attention, and add the result to the input (residual connection).\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        \n",
    "        # Apply another layer normalization, then process through the MLP, and add the result to the output\n",
    "        # of the previous self-attention layer (residual connection).\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class JXTConfig:\n",
    "\n",
    "    block_size: int = 1024 # max sequence length\n",
    "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of heads\n",
    "    n_embd: int = 768 # embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JXT(nn.Module):\n",
    "    \"\"\"\n",
    "    The JXT class encapsulates the JXT architecture, configuring the model with token and position embeddings,\n",
    "    multiple transformer blocks, and a final output layer to produce logits over a vocabulary. This class is \n",
    "    designed for autoregressive language modeling tasks where each prediction depends only on previous tokens.\n",
    "\n",
    "    Attributes:\n",
    "        config (JXTConfig): Configuration object containing model hyperparameters such as the number of layers,\n",
    "                            embedding dimension, vocabulary size, and maximum sequence length.\n",
    "        transformer (nn.ModuleDict): Contains the embeddings and transformer blocks.\n",
    "        lm_head (nn.Linear): Linear transformation applied to the outputs of the transformer blocks to\n",
    "                             produce logits corresponding to the probability distribution over the vocabulary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initializes the JXT model with the specified configuration.\n",
    "\n",
    "        Args:\n",
    "            config (JXTConfig): A configuration object specifying model dimensions and architecture parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Initializing embeddings and transformer blocks.\n",
    "        self.transformer = nn.ModuleDict({\n",
    "            'wte': nn.Embedding(config.vocab_size, config.n_embd),  # Token embeddings.\n",
    "            'wpe': nn.Embedding(config.block_size, config.n_embd),  # Position embeddings.\n",
    "            'h': nn.ModuleList([Block(config) for _ in range(config.n_layer)]),  # Transformer blocks.\n",
    "            'ln_f': nn.LayerNorm(config.n_embd),  # Final layer normalization.\n",
    "        })\n",
    "\n",
    "        # Output layer that projects the final transformer outputs to the vocabulary size.\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the JXT model.\n",
    "\n",
    "        Args:\n",
    "            idx (torch.Tensor): Tensor containing token indices of the input sequence (batch_size, sequence_length).\n",
    "\n",
    "        Returns:\n",
    "            logits (torch.Tensor): The logits predicting the next token in the sequence\n",
    "                                            (batch_size, sequence_length, vocab_size).\n",
    "        \"\"\"\n",
    "        B, T = idx.size()\n",
    "        # Ensure the input does not exceed the configured maximum sequence length.\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "\n",
    "        # Generate position embeddings and add them to the token embeddings.\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        # Pass the combined embeddings through each transformer block.\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        # Apply the final layer normalization.\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        # Generate logits for each token in the vocabulary.\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        print(f\"loading weights from pretrained GPT: {model_type}\")\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized JXT model\n",
    "        config = JXTConfig(**config_args)\n",
    "        model = JXT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(21, 768)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "vocab_size = len(tokenizer.token_to_id)\n",
    "model.resize_token_embeddings(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = train_df['sequence'].tolist()\n",
    "train_gene_ids = train_df['ensg'].tolist()\n",
    "\n",
    "valid_sequences = val_df['sequence'].tolist()\n",
    "valid_gene_ids = val_df['ensg'].tolist()\n",
    "\n",
    "test_sequences = test_df['sequence'].tolist()\n",
    "test_gene_ids = test_df['ensg'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GeneSequenceDataset(train_gene_ids, train_sequences, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "val_dataset = GeneSequenceDataset(valid_gene_ids, valid_sequences, tokenizer)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "test_dataset = GeneSequenceDataset(test_gene_ids, test_sequences, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def custom_loss(model_output, labels, tokenizer):\n",
    "    \n",
    "    outputs = model_output.logits\n",
    "\n",
    "    # Find indices of [SOS] and [EOS] tokens\n",
    "    sos_id = tokenizer.token_to_id['[SOS]']\n",
    "    eos_id = tokenizer.token_to_id['[EOS]']\n",
    "\n",
    "    loss = 0\n",
    "    batch_size = outputs.size(0)\n",
    "    for i in range(batch_size):\n",
    "        # Extract the sequence between [SOS] and [EOS]\n",
    "        start = (labels[i] == sos_id).nonzero(as_tuple=True)[0]\n",
    "        end = (labels[i] == eos_id).nonzero(as_tuple=True)[0]\n",
    "        if start.nelement() == 0 or end.nelement() == 0:\n",
    "            continue  # Skip if [SOS] or [EOS] not found\n",
    "        if start.item() >= end.item():\n",
    "            continue  # Ensure valid range\n",
    "        # Calculate loss only within the [SOS] and [EOS] range\n",
    "        relevant_outputs = outputs[i, start:end, :]\n",
    "        relevant_labels = labels[i, start:end]\n",
    "        loss += F.cross_entropy(relevant_outputs, relevant_labels, reduction='mean')\n",
    "    return loss / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(logits, labels):\n",
    "    preds = torch.argmax(logits, dim=-1)\n",
    "    mask = (labels != tokenizer.token_to_id['[PAD]']).float()\n",
    "    correct = ((preds == labels) * mask).sum().item()\n",
    "    total = mask.sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, tokenizer, optimizer, epochs, device):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "        train_steps = len(train_loader)\n",
    "        val_steps = len(val_loader)\n",
    "\n",
    "        train_progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\", unit=\"batch\")\n",
    "        \n",
    "        for step, batch in enumerate(train_progress):\n",
    "            inputs, labels = batch[:, :-1], batch[:, 1:]  # Shifted for predicting the next token\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = custom_loss(outputs, labels, tokenizer)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            accuracy = calculate_accuracy(outputs.logits, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_accuracy += accuracy\n",
    "\n",
    "            train_progress.set_postfix({\"Step\": step + 1, \"Loss\": loss.item(), \"Accuracy\": accuracy})\n",
    "\n",
    "        avg_train_loss = total_loss / train_steps\n",
    "        avg_train_accuracy = total_accuracy / train_steps\n",
    "        print(f\"Epoch {epoch + 1}, Average Training Loss: {avg_train_loss}, Average Training Accuracy: {avg_train_accuracy}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_accuracy = 0\n",
    "        val_progress = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\", unit=\"batch\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for step, batch in enumerate(val_progress):\n",
    "                inputs, labels = batch[:, :-1], batch[:, 1:]\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = custom_loss(outputs, labels, tokenizer)\n",
    "                accuracy = calculate_accuracy(outputs.logits, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                val_accuracy += accuracy\n",
    "\n",
    "                val_progress.set_postfix({\"Step\": step + 1, \"Loss\": loss.item(), \"Accuracy\": accuracy})\n",
    "\n",
    "        avg_val_loss = val_loss / val_steps\n",
    "        avg_val_accuracy = val_accuracy / val_steps\n",
    "        print(f\"Epoch {epoch + 1}, Average Validation Loss: {avg_val_loss}, Average Validation Accuracy: {avg_val_accuracy}\")\n",
    "\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Training:   0%|          | 0/2 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x105d96e00>> tensor([[ 8,  9, 10,  ...,  6,  6,  3],\n",
      "        [ 8,  9, 10,  ...,  6,  6,  3],\n",
      "        [ 8,  9, 10,  ...,  6,  6,  3],\n",
      "        ...,\n",
      "        [ 8,  9, 10,  ...,  6,  6,  3],\n",
      "        [ 8,  9, 10,  ...,  6,  6,  3],\n",
      "        [ 8,  9, 10,  ...,  6,  6,  3]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "train(model, train_loader, val_loader, tokenizer, optimizer, epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def generate_sgRNA_sequence(model, tokenizer, gene_id, max_length=20, num_sequences=5, device='cpu'):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        # Prepare the input with the gene ID and start token\n",
    "        input_tokens = f\"[ID]{gene_id}[SOS]\"\n",
    "        input_ids = tokenizer.encode(input_tokens)\n",
    "        \n",
    "        input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
    "\n",
    "        # Beam search parameters\n",
    "        beam_size = num_sequences\n",
    "        sequences = [[input_ids[:], 0]]  # List of [sequence, score]\n",
    "\n",
    "        for _ in range(max_length):  # Limit maximum generation length\n",
    "            all_candidates = []\n",
    "            for seq, score in sequences:\n",
    "                input_tensor = torch.tensor([seq], dtype=torch.long).to(device)\n",
    "                output = model(input_tensor)\n",
    "                predictions = output.logits[0, -1, :]  # Get logits for the last token\n",
    "                probs = F.softmax(predictions, dim=-1)\n",
    "                top_k_probs, top_k_ids = probs.topk(beam_size)  # Get top k probabilities and token IDs\n",
    "                \n",
    "                for i in range(beam_size):\n",
    "                    candidate = [seq + [top_k_ids[i].item()], score - torch.log(top_k_probs[i]).item()]\n",
    "                    all_candidates.append(candidate)\n",
    "\n",
    "            # Order all candidates by score\n",
    "            ordered = sorted(all_candidates, key=lambda x: x[1])\n",
    "            sequences = ordered[:beam_size]\n",
    "\n",
    "            # Stop if all sequences end with [EOS]\n",
    "            if all(tokenizer.token_to_id['[EOS]'] in seq for seq, score in sequences):\n",
    "                break\n",
    "\n",
    "        # Decode the top sequences\n",
    "        top_sequences = []\n",
    "        for seq, score in sequences:\n",
    "            if tokenizer.token_to_id['[EOS]'] in seq:\n",
    "                end_index = seq.index(tokenizer.token_to_id['[EOS]'])\n",
    "            else:\n",
    "                end_index = len(seq)\n",
    "            trimmed_seq = seq[len(input_ids):end_index]\n",
    "            generated_sequence = tokenizer.decode(trimmed_seq)\n",
    "            top_sequences.append(generated_sequence)\n",
    "\n",
    "        return top_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sgRNA Sequence: ['GGGGGGGGGGGGGGGGGGGG', 'GGGGGGGGGGTGGGGGGGGG', 'GGGGGGGGGGGGGTGGGGGG', 'GGGGGGGGGGAGGGGGGGGG', 'GGGGGGGGGAGGGGGGGGGG']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "gene_id = \"ENSG00000148584\"\n",
    "generated_sgRNA = generate_sgRNA_sequence(model, tokenizer, gene_id)\n",
    "print(\"Generated sgRNA Sequence:\", generated_sgRNA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-2 Predicting Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SgRNARegressionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(SgRNARegressionModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        embedded = self.embedding(sequences)\n",
    "        embedded = embedded.mean(dim=1)  # Reduce dimensionality\n",
    "        hidden = self.relu(self.linear1(embedded))\n",
    "        output = self.linear2(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SgRNADataset(Dataset):\n",
    "    def __init__(self, sequences, log2fc):\n",
    "        self.sequences = sequences\n",
    "        self.log2fc = log2fc\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = torch.tensor(self.sequences[idx], dtype=torch.long)\n",
    "        log2fc = torch.tensor(self.log2fc[idx], dtype=torch.float)\n",
    "        return sequence, log2fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data\n",
    "sequences = [[5, 3, 2, 7], [4, 3, 2, 8], [5, 3, 3, 7]]  # Tokenized sequences\n",
    "log2fc = [1.2, -0.5, 0.3]  # Corresponding log2fc values\n",
    "\n",
    "dataset = SgRNADataset(sequences, log2fc)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample training run\n",
    "vocab_size = 21\n",
    "embedding_dim = 20\n",
    "hidden_dim = 50\n",
    "output_dim = 1\n",
    "\n",
    "model = SgRNARegressionModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for sequences, log2fc in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(sequences)\n",
    "        loss = criterion(predictions.squeeze(), log2fc)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    train_loss, val_loss, train_accuracy, val_accuracy = [], [], [], []\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            train_loss.append(data['train_loss'])\n",
    "            val_loss.append(data['val_loss'])\n",
    "            train_accuracy.append(data['train_accuracy'])\n",
    "            val_accuracy.append(data['val_accuracy'])\n",
    "    \n",
    "    return train_loss, val_loss, train_accuracy, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(train_loss, val_loss, train_accuracy, val_accuracy):\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # plt.subplot(1, 2, 1)\n",
    "    # plt.plot(epochs, train_loss, 'b-', label='Training Loss')\n",
    "    # plt.plot(epochs, val_loss, 'r-', label='Validation Loss')\n",
    "    # plt.title('Training and Validation Loss')\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.ylabel('Loss')\n",
    "    # plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracy, 'b-', label='Training Accuracy')\n",
    "    plt.plot(epochs, val_accuracy, 'r-', label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy: GPT2 128M Model')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the log file\n",
    "file_path = '../src/results/logs/2024-07-19.log'\n",
    "\n",
    "# Load data from log file\n",
    "train_loss, val_loss, train_accuracy, val_accuracy = load_data(file_path)\n",
    "\n",
    "# Plot the metrics\n",
    "plot_metrics(train_loss, val_loss, train_accuracy, val_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

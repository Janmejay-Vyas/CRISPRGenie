{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE\n",
    "Try a multimodal approach as follows:\n",
    "1. Train a simple model to predict sgRNA sequences\n",
    "2. Next, fine tuning: Using the Gene ID and its respective sequences, try to fine tune the model to predict the sgRNA sequences based on the gene ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a causal self-attention mechanism which is a fundamental component of transformer models\n",
    "    designed for sequence processing tasks where the model should not have future insight. This module \n",
    "    ensures that the predictions for a particular position are dependent only on the known outputs at \n",
    "    previous positions.\n",
    "\n",
    "    Attributes:\n",
    "        c_attn (nn.Linear): Linear layer that projects input embeddings into queries, keys, and values.\n",
    "        c_proj (nn.Linear): Linear layer that projects the output of the attention mechanism back to\n",
    "                            the dimension of embeddings.\n",
    "        bias (torch.Tensor): Buffer that applies a triangular mask to ensure attention is only applied\n",
    "                             to preceding positions, preserving causality.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initializes the CausalSelfAttention layer with specific configuration.\n",
    "\n",
    "        Args:\n",
    "            config: A configuration object containing attributes like `n_embd` (embedding size),\n",
    "                    `n_head` (number of attention heads), and `block_size` (sequence length).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Ensuring the embedding size is divisible by the number of heads for even split.\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        # Linear transformation that outputs triple the embedding dimension to split into\n",
    "        # queries, keys, and values.\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "\n",
    "        # Linear transformation for the output of the attention computation.\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "\n",
    "        # Store the number of attention heads and the embedding dimension per head.\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "        # Register a buffer for the triangular mask that prevents attending to future positions.\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                         .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the causal self-attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, sequence_length, embedding_dim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after processing with causal self-attention.\n",
    "        \"\"\"\n",
    "        # Unpack the dimensions of the input tensor.\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # Pass the input through the attention projection layer to get combined query, key, value tensors.\n",
    "        qkv = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "\n",
    "        # Split and reshape the combined QKV tensor into individual Q, K, V tensors and transpose\n",
    "        # for multi-head attention computation.\n",
    "        q, k, v = [tensor.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) for tensor in qkv]\n",
    "\n",
    "        # Compute the attention scores, apply scaling for stability, and use the mask to enforce causality.\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "\n",
    "        # Apply softmax to convert scores to probabilities and compute the weighted sum of values.\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = (att @ v).transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        # Project the output back to the embedding dimension and return.\n",
    "        return self.c_proj(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A multilayer perceptron (MLP) module used within transformer blocks as a position-wise\n",
    "    feed-forward network. This module is a simple neural network for transforming the \n",
    "    representation at every position independently in the sequence.\n",
    "\n",
    "    Attributes:\n",
    "        c_fc (nn.Linear): The first linear layer that expands the input dimension.\n",
    "        gelu (nn.GELU): Gaussian Error Linear Unit (GELU) activation function, which\n",
    "                        allows the model to include non-linearity and helps in learning\n",
    "                        more complex patterns. This version uses the 'tanh' approximation\n",
    "                        for faster computation.\n",
    "        c_proj (nn.Linear): The second linear layer that projects the output back to \n",
    "                            the original embedding dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initializes the MLP module with specified configurations.\n",
    "\n",
    "        Args:\n",
    "            config: A configuration object containing `n_embd`, the size of the input\n",
    "                    and output embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # First linear layer that increases dimensionality 4x to allow more complex interactions.\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        \n",
    "        # GELU activation function with 'tanh' approximation.\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        \n",
    "        # Second linear layer that reduces dimensionality back to the original size.\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the MLP module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor to the MLP with shape (batch_size, sequence_length, n_embd).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after processing through two linear layers\n",
    "                          and a GELU activation function, with the same shape as input.\n",
    "        \"\"\"\n",
    "        # Pass the input through the first linear layer and then apply the GELU activation function.\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        \n",
    "        # Finally, pass the activated output through the second linear layer to match the original embedding size.\n",
    "        x = self.c_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Represents a single Transformer block, which is a fundamental component of the Transformer architecture.\n",
    "    Each block sequentially applies layer normalization, a causal self-attention mechanism, another layer normalization,\n",
    "    and a multilayer perceptron (MLP). The architecture follows a typical pattern used in JXT models,\n",
    "    implementing a residual connection around each of the two main sub-layers (self-attention and MLP).\n",
    "\n",
    "    Attributes:\n",
    "        ln_1 (nn.LayerNorm): Layer normalization applied before the self-attention mechanism.\n",
    "        attn (CausalSelfAttention): The causal self-attention module, ensuring that the predictions\n",
    "                                    for a position are dependent only on the known outputs at previous positions.\n",
    "        ln_2 (nn.LayerNorm): Layer normalization applied before the MLP.\n",
    "        mlp (MLP): The multilayer perceptron module that processes the output of the attention mechanism.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initializes the Transformer block with specified configurations.\n",
    "\n",
    "        Args:\n",
    "            config: A configuration object containing necessary parameters like `n_embd`, which is used\n",
    "                    to set the dimensionality of the layer normalization and to configure the attention and MLP modules.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Layer normalization that normalizes the embeddings before the self-attention layer.\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        \n",
    "        # The self-attention mechanism defined in the CausalSelfAttention class.\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        \n",
    "        # Layer normalization that normalizes the output of the attention mechanism before passing it to the MLP.\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        \n",
    "        # The MLP that further processes the output from the attention mechanism.\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass through the Transformer block.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor to the block with shape (batch_size, sequence_length, n_embd).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor from the block, which has the same shape as the input.\n",
    "                          This output can be fed into subsequent blocks in a Transformer model.\n",
    "        \"\"\"\n",
    "        # Apply layer normalization, then self-attention, and add the result to the input (residual connection).\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        \n",
    "        # Apply another layer normalization, then process through the MLP, and add the result to the output\n",
    "        # of the previous self-attention layer (residual connection).\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # max sequence length\n",
    "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of heads\n",
    "    n_embd: int = 768 # embedding dimension\n",
    "    sos_token_id: int = 1\n",
    "    eos_token_id: int = 2\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict({\n",
    "            'wte': nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            'wpe': nn.Embedding(config.block_size, config.n_embd),\n",
    "            'h': nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            'ln_f': nn.LayerNorm(config.n_embd),\n",
    "        })\n",
    "        self.ffn_head = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, config.n_embd),  # Adjust dimensions as needed\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.n_embd, config.n_embd)\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.size()\n",
    "        #print(f\"Input idx shape: {idx.shape}\")\n",
    "\n",
    "        # Assuming 'sos_token_id' is defined in config\n",
    "        sos_positions = (idx == self.config.sos_token_id).nonzero(as_tuple=True)\n",
    "        gene_id_end_idx = sos_positions[1].min()  # Index of the first SOS token\n",
    "        gene_id_idx = idx[:, :gene_id_end_idx]\n",
    "        sequence_idx = idx[:, gene_id_end_idx+1:]  # Exclude SOS token from sequence\n",
    "\n",
    "        # Process gene_id through FFN\n",
    "        gene_id_emb = self.transformer.wte(gene_id_idx)\n",
    "        gene_id_context = self.ffn_head(gene_id_emb.mean(dim=1))  # Using mean to collapse gene_id embeddings\n",
    "\n",
    "        # Process sequence\n",
    "        pos = torch.arange(0, sequence_idx.size(1), dtype=torch.long, device=idx.device)\n",
    "        pos_emb = self.transformer.wpe(pos)  # Position embeddings\n",
    "        tok_emb = self.transformer.wte(sequence_idx)  # Token embeddings\n",
    "        x = tok_emb + pos_emb.unsqueeze(0)  # Broadcast position embeddings\n",
    "        x += gene_id_context.unsqueeze(1)  # Add gene ID context to each token position in the sequence\n",
    "\n",
    "        #print(f\"Combined input shape after context addition: {x.shape}\")\n",
    "\n",
    "        # Forward the blocks of the transformer\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        # Final layer norm and classifier\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        logits = logits.view(-1, self.config.vocab_size)\n",
    "        #print(f\"Logits shape after view: {logits.shape}\")\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            # Assuming the targets are sliced in the data preparation or dataloader to match the sequence indices\n",
    "            targets = targets[:, gene_id_end_idx+1:].contiguous().view(-1)  # Adjust target to only include sequence part\n",
    "            #print(f\"Targets shape after view: {targets.shape}\")\n",
    "            if logits.size(0) != targets.size(0):\n",
    "                raise ValueError(f\"Logits and targets size mismatch: {logits.size(0)} vs {targets.size(0)}\")\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    def __init__(self):\n",
    "        self.token_to_id = {\n",
    "            '[PAD]': 0, '[SOS]': 1, '[EOS]': 2,\n",
    "            'A': 3, 'T': 4, 'G': 5, 'C': 6,\n",
    "            '[ID]': 7,  # Special token for gene ID start\n",
    "            '0': 8, '1': 9, '2': 10, '3': 11, '4': 12,\n",
    "            '5': 13, '6': 14, '7': 15, '8': 16, '9': 17,\n",
    "            '[PAD]': 18,\n",
    "        }\n",
    "        self.id_to_token = {v: k for k, v in self.token_to_id.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = []\n",
    "        i = 0\n",
    "        while i < len(text):\n",
    "            if text[i:i+4] == 'ENSG':  # Recognize the ENSG gene ID prefix\n",
    "                tokens.append('[ID]')\n",
    "                i += 4  # Skip the 'ENSG'\n",
    "            elif text[i] == '[':  # Special tokens handling\n",
    "                special_token_end = text.find(']', i)\n",
    "                tokens.append(text[i:special_token_end+1])\n",
    "                i = special_token_end + 1\n",
    "            else:\n",
    "                tokens.append(text[i])\n",
    "                i += 1\n",
    "        return [self.token_to_id.get(token, self.token_to_id['[PAD]']) for token in tokens]\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.token_to_id)\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        \"\"\" Convert a list of token IDs back to a string. \"\"\"\n",
    "        return ''.join(self.id_to_token.get(token_id, '') for token_id in token_ids)\n",
    "    \n",
    "    def is_special_token(self, token_ids):\n",
    "        \"\"\"Return if a token is a special token\"\"\"\n",
    "        return [token_ids > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderLite:\n",
    "    def __init__(self, gene_ids, sequences, B, T, tokenizer):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.tokenizer = tokenizer\n",
    "        self.current_position = 0\n",
    "        self.pad_token_id = tokenizer.token_to_id['[PAD]']\n",
    "        \n",
    "        # Prepare data\n",
    "        self.batches = []\n",
    "        for i in range(0, len(gene_ids), B):\n",
    "            batch_gene_ids = gene_ids[i:i+B]\n",
    "            batch_sequences = sequences[i:i+B]\n",
    "            max_len = 0\n",
    "            batch_encoded = []\n",
    "            \n",
    "            # Encode and find the max length in the current batch\n",
    "            for gene_id, seq in zip(batch_gene_ids, batch_sequences):\n",
    "                formatted_seq = f'{gene_id}[SOS]{seq}[EOS]'\n",
    "                encoded = tokenizer.encode(formatted_seq)\n",
    "                max_len = max(max_len, len(encoded))\n",
    "                batch_encoded.append(encoded)\n",
    "            \n",
    "            # Pad sequences in the current batch to the max length\n",
    "            padded_batch = []\n",
    "            for encoded in batch_encoded:\n",
    "                padded_length = max_len - len(encoded)\n",
    "                padded_seq = encoded + [self.pad_token_id] * padded_length\n",
    "                padded_batch.append(padded_seq)\n",
    "            \n",
    "            # Add the padded batch to batches\n",
    "            self.batches.append(torch.tensor(padded_batch, dtype=torch.long))\n",
    "    \n",
    "    def next_batch(self):\n",
    "        if self.current_position >= len(self.batches):\n",
    "            self.current_position = 0  # Reset for next epoch\n",
    "            return None, None  # No more data\n",
    "        \n",
    "        batch = self.batches[self.current_position]\n",
    "        self.current_position += 1\n",
    "        \n",
    "        # Prepare targets by rolling tensors\n",
    "        targets = batch.roll(-1, dims=1)\n",
    "        return batch, targets\n",
    "\n",
    "    def view_data(self):\n",
    "        # Return a readable format of the data (decoding back from token IDs to text)\n",
    "        readable_data = []\n",
    "        for batch in self.batches:\n",
    "            for sequence in batch:\n",
    "                decoded_sequence = [self.tokenizer.decode([token]) for token in sequence.tolist()]\n",
    "                readable_data.append(decoded_sequence)\n",
    "        return readable_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_return_sequences = 5\n",
    "max_length = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>chr</th>\n",
       "      <th>strand</th>\n",
       "      <th>pubmed</th>\n",
       "      <th>cellline</th>\n",
       "      <th>condition</th>\n",
       "      <th>sequence</th>\n",
       "      <th>symbol</th>\n",
       "      <th>ensg</th>\n",
       "      <th>log2fc</th>\n",
       "      <th>rc_initial</th>\n",
       "      <th>rc_final</th>\n",
       "      <th>effect</th>\n",
       "      <th>cas</th>\n",
       "      <th>screentype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50844073</td>\n",
       "      <td>50844096</td>\n",
       "      <td>10</td>\n",
       "      <td>+</td>\n",
       "      <td>26472758</td>\n",
       "      <td>Jiyoye</td>\n",
       "      <td>viability</td>\n",
       "      <td>GCAGCATCCCAACCAGGTGGAGG</td>\n",
       "      <td>A1CF</td>\n",
       "      <td>ENSG00000148584</td>\n",
       "      <td>0.315907</td>\n",
       "      <td>{260}</td>\n",
       "      <td>{244}</td>\n",
       "      <td>2</td>\n",
       "      <td>hSpCas9</td>\n",
       "      <td>negative selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50814011</td>\n",
       "      <td>50814034</td>\n",
       "      <td>10</td>\n",
       "      <td>-</td>\n",
       "      <td>26472758</td>\n",
       "      <td>Jiyoye</td>\n",
       "      <td>viability</td>\n",
       "      <td>GCGGGAGTGAGAGGACTGGGCGG</td>\n",
       "      <td>A1CF</td>\n",
       "      <td>ENSG00000148584</td>\n",
       "      <td>2.144141</td>\n",
       "      <td>{17}</td>\n",
       "      <td>{59}</td>\n",
       "      <td>9</td>\n",
       "      <td>hSpCas9</td>\n",
       "      <td>negative selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50836111</td>\n",
       "      <td>50836134</td>\n",
       "      <td>10</td>\n",
       "      <td>+</td>\n",
       "      <td>26472758</td>\n",
       "      <td>Jiyoye</td>\n",
       "      <td>viability</td>\n",
       "      <td>ATGACTCTCATACTCCACGAAGG</td>\n",
       "      <td>A1CF</td>\n",
       "      <td>ENSG00000148584</td>\n",
       "      <td>1.426034</td>\n",
       "      <td>{75}</td>\n",
       "      <td>{153}</td>\n",
       "      <td>8</td>\n",
       "      <td>hSpCas9</td>\n",
       "      <td>negative selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50836095</td>\n",
       "      <td>50836118</td>\n",
       "      <td>10</td>\n",
       "      <td>-</td>\n",
       "      <td>26472758</td>\n",
       "      <td>Jiyoye</td>\n",
       "      <td>viability</td>\n",
       "      <td>GAGTCATCGAGCAGCTGCCATGG</td>\n",
       "      <td>A1CF</td>\n",
       "      <td>ENSG00000148584</td>\n",
       "      <td>1.550133</td>\n",
       "      <td>{47}</td>\n",
       "      <td>{105}</td>\n",
       "      <td>8</td>\n",
       "      <td>hSpCas9</td>\n",
       "      <td>negative selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50816234</td>\n",
       "      <td>50816257</td>\n",
       "      <td>10</td>\n",
       "      <td>-</td>\n",
       "      <td>26472758</td>\n",
       "      <td>Jiyoye</td>\n",
       "      <td>viability</td>\n",
       "      <td>AGTCACCCTAGCAAAACCAGTGG</td>\n",
       "      <td>A1CF</td>\n",
       "      <td>ENSG00000148584</td>\n",
       "      <td>0.382513</td>\n",
       "      <td>{58}</td>\n",
       "      <td>{57}</td>\n",
       "      <td>3</td>\n",
       "      <td>hSpCas9</td>\n",
       "      <td>negative selection</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      start       end chr strand    pubmed cellline  condition  \\\n",
       "0  50844073  50844096  10      +  26472758   Jiyoye  viability   \n",
       "1  50814011  50814034  10      -  26472758   Jiyoye  viability   \n",
       "2  50836111  50836134  10      +  26472758   Jiyoye  viability   \n",
       "3  50836095  50836118  10      -  26472758   Jiyoye  viability   \n",
       "4  50816234  50816257  10      -  26472758   Jiyoye  viability   \n",
       "\n",
       "                  sequence symbol             ensg    log2fc rc_initial  \\\n",
       "0  GCAGCATCCCAACCAGGTGGAGG   A1CF  ENSG00000148584  0.315907      {260}   \n",
       "1  GCGGGAGTGAGAGGACTGGGCGG   A1CF  ENSG00000148584  2.144141       {17}   \n",
       "2  ATGACTCTCATACTCCACGAAGG   A1CF  ENSG00000148584  1.426034       {75}   \n",
       "3  GAGTCATCGAGCAGCTGCCATGG   A1CF  ENSG00000148584  1.550133       {47}   \n",
       "4  AGTCACCCTAGCAAAACCAGTGG   A1CF  ENSG00000148584  0.382513       {58}   \n",
       "\n",
       "  rc_final  effect      cas          screentype  \n",
       "0    {244}       2  hSpCas9  negative selection  \n",
       "1     {59}       9  hSpCas9  negative selection  \n",
       "2    {153}       8  hSpCas9  negative selection  \n",
       "3    {105}       8  hSpCas9  negative selection  \n",
       "4     {57}       3  hSpCas9  negative selection  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/GenomeCRISPR.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CustomTokenizer()\n",
    "train_loader = DataLoaderLite(df['ensg'],df['sequence'], B=4, T=32, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(GPTConfig()).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 10.994247436523438\n",
      "step 1, loss: 10.946769714355469\n",
      "step 2, loss: 10.846275329589844\n",
      "step 3, loss: 10.668484687805176\n",
      "step 4, loss: 10.578746795654297\n",
      "step 5, loss: 10.516585350036621\n",
      "step 6, loss: 10.400824546813965\n",
      "step 7, loss: 10.247157096862793\n",
      "step 8, loss: 10.16037368774414\n",
      "step 9, loss: 10.146445274353027\n",
      "step 10, loss: 9.870000839233398\n",
      "step 11, loss: 10.06450366973877\n",
      "step 12, loss: 9.885464668273926\n",
      "step 13, loss: 9.647636413574219\n",
      "step 14, loss: 9.538131713867188\n",
      "step 15, loss: 9.528976440429688\n",
      "step 16, loss: 9.46561336517334\n",
      "step 17, loss: 9.39891242980957\n",
      "step 18, loss: 9.37364387512207\n",
      "step 19, loss: 9.336405754089355\n",
      "step 20, loss: 9.190179824829102\n",
      "step 21, loss: 8.909993171691895\n",
      "step 22, loss: 9.193188667297363\n",
      "step 23, loss: 9.118147850036621\n",
      "step 24, loss: 8.874943733215332\n",
      "step 25, loss: 8.61152458190918\n",
      "step 26, loss: 8.61601734161377\n",
      "step 27, loss: 8.696346282958984\n",
      "step 28, loss: 8.51191520690918\n",
      "step 29, loss: 8.504636764526367\n",
      "step 30, loss: 8.353549003601074\n",
      "step 31, loss: 8.283648490905762\n",
      "step 32, loss: 8.246230125427246\n",
      "step 33, loss: 8.109079360961914\n",
      "step 34, loss: 8.1550931930542\n",
      "step 35, loss: 8.002852439880371\n",
      "step 36, loss: 7.767568588256836\n",
      "step 37, loss: 7.727777004241943\n",
      "step 38, loss: 7.672709941864014\n",
      "step 39, loss: 7.6939520835876465\n",
      "step 40, loss: 7.3376240730285645\n",
      "step 41, loss: 7.42780065536499\n",
      "step 42, loss: 7.388883113861084\n",
      "step 43, loss: 7.348185062408447\n",
      "step 44, loss: 7.073864459991455\n",
      "step 45, loss: 7.094974040985107\n",
      "step 46, loss: 7.204214572906494\n",
      "step 47, loss: 6.928951263427734\n",
      "step 48, loss: 6.894399642944336\n",
      "step 49, loss: 6.7722015380859375\n",
      "step 50, loss: 6.585126876831055\n",
      "step 51, loss: 6.616063594818115\n",
      "step 52, loss: 6.679854869842529\n",
      "step 53, loss: 6.5139312744140625\n",
      "step 54, loss: 6.6102614402771\n",
      "step 55, loss: 6.509762287139893\n",
      "step 56, loss: 6.5396342277526855\n",
      "step 57, loss: 6.383243083953857\n",
      "step 58, loss: 6.4339375495910645\n",
      "step 59, loss: 6.4957194328308105\n",
      "step 60, loss: 6.159196853637695\n",
      "step 61, loss: 6.178982257843018\n",
      "step 62, loss: 5.932098865509033\n",
      "step 63, loss: 6.046792507171631\n",
      "step 64, loss: 5.897297382354736\n",
      "step 65, loss: 5.8308539390563965\n",
      "step 66, loss: 5.639169216156006\n",
      "step 67, loss: 6.1566081047058105\n",
      "step 68, loss: 5.424779891967773\n",
      "step 69, loss: 5.666584014892578\n",
      "step 70, loss: 5.436777114868164\n",
      "step 71, loss: 5.508683681488037\n",
      "step 72, loss: 5.329849720001221\n",
      "step 73, loss: 5.412193775177002\n",
      "step 74, loss: 5.264927387237549\n",
      "step 75, loss: 5.3403096199035645\n",
      "step 76, loss: 5.265181064605713\n",
      "step 77, loss: 5.392812252044678\n",
      "step 78, loss: 5.358763217926025\n",
      "step 79, loss: 5.169384956359863\n",
      "step 80, loss: 5.419807434082031\n",
      "step 81, loss: 5.037529945373535\n",
      "step 82, loss: 5.376964569091797\n",
      "step 83, loss: 5.040010929107666\n",
      "step 84, loss: 4.913530349731445\n",
      "step 85, loss: 5.094465732574463\n",
      "step 86, loss: 4.673844337463379\n",
      "step 87, loss: 4.908000469207764\n",
      "step 88, loss: 4.751173973083496\n",
      "step 89, loss: 4.843081951141357\n",
      "step 90, loss: 4.847972393035889\n",
      "step 91, loss: 5.162615776062012\n",
      "step 92, loss: 4.708008289337158\n",
      "step 93, loss: 4.973944664001465\n",
      "step 94, loss: 4.612191677093506\n",
      "step 95, loss: 4.804739475250244\n",
      "step 96, loss: 4.5890421867370605\n",
      "step 97, loss: 4.750738143920898\n",
      "step 98, loss: 4.699423789978027\n",
      "step 99, loss: 4.6054511070251465\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-6)\n",
    "for i in range(100):\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to('cpu'), y.to('cpu')\n",
    "    # print(\"Input shape:\", x.shape)  # Should match (B, T) for input\n",
    "    # print(\"Target shape:\", y.shape)  # Should be (B, T) initially, reshaped later\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()        \n",
    "    print(f\"step {i}, loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sequence: GGGGGGGGGAGGGG\n",
      "Actual Sequence: GGATACGATACATGGA\n",
      "Levenshtein Distance: 10\n",
      "Accuracy: 0.375\n",
      "Step 0:\n",
      "  G (Prob: 0.0008)\n",
      "  C (Prob: 0.0004)\n",
      "   (Prob: 0.0002)\n",
      "   (Prob: 0.0002)\n",
      "   (Prob: 0.0002)\n",
      "Step 1:\n",
      "  G (Prob: 0.0021)\n",
      "  A (Prob: 0.0013)\n",
      "  C (Prob: 0.0003)\n",
      "   (Prob: 0.0002)\n",
      "   (Prob: 0.0002)\n",
      "Step 2:\n",
      "  G (Prob: 0.0020)\n",
      "  A (Prob: 0.0008)\n",
      "  C (Prob: 0.0006)\n",
      "   (Prob: 0.0003)\n",
      "   (Prob: 0.0002)\n",
      "Step 3:\n",
      "  G (Prob: 0.0005)\n",
      "  A (Prob: 0.0005)\n",
      "  C (Prob: 0.0005)\n",
      "   (Prob: 0.0002)\n",
      "   (Prob: 0.0002)\n",
      "Step 4:\n",
      "  G (Prob: 0.0019)\n",
      "  A (Prob: 0.0004)\n",
      "  C (Prob: 0.0003)\n",
      "   (Prob: 0.0002)\n",
      "   (Prob: 0.0002)\n",
      "Step 5:\n",
      "  G (Prob: 0.0012)\n",
      "  A (Prob: 0.0008)\n",
      "  C (Prob: 0.0005)\n",
      "  T (Prob: 0.0002)\n",
      "   (Prob: 0.0002)\n",
      "Step 6:\n",
      "  G (Prob: 0.0010)\n",
      "  A (Prob: 0.0007)\n",
      "  T (Prob: 0.0003)\n",
      "  C (Prob: 0.0003)\n",
      "   (Prob: 0.0002)\n",
      "Step 7:\n",
      "  G (Prob: 0.0015)\n",
      "  C (Prob: 0.0008)\n",
      "  A (Prob: 0.0004)\n",
      "  T (Prob: 0.0003)\n",
      "   (Prob: 0.0002)\n",
      "Step 8:\n",
      "  G (Prob: 0.0047)\n",
      "  A (Prob: 0.0007)\n",
      "  C (Prob: 0.0003)\n",
      "  T (Prob: 0.0002)\n",
      "   (Prob: 0.0002)\n",
      "Step 9:\n",
      "  A (Prob: 0.0024)\n",
      "  G (Prob: 0.0022)\n",
      "  C (Prob: 0.0003)\n",
      "   (Prob: 0.0002)\n",
      "   (Prob: 0.0002)\n",
      "Step 10:\n",
      "  G (Prob: 0.0029)\n",
      "  A (Prob: 0.0009)\n",
      "  C (Prob: 0.0005)\n",
      "   (Prob: 0.0002)\n",
      "   (Prob: 0.0002)\n",
      "Step 11:\n",
      "  G (Prob: 0.0029)\n",
      "  A (Prob: 0.0010)\n",
      "  T (Prob: 0.0004)\n",
      "  C (Prob: 0.0003)\n",
      "   (Prob: 0.0002)\n",
      "Step 12:\n",
      "  G (Prob: 0.0035)\n",
      "  A (Prob: 0.0004)\n",
      "  T (Prob: 0.0004)\n",
      "  C (Prob: 0.0003)\n",
      "   (Prob: 0.0002)\n",
      "Step 13:\n",
      "  G (Prob: 0.0010)\n",
      "  A (Prob: 0.0007)\n",
      "  C (Prob: 0.0005)\n",
      "   (Prob: 0.0003)\n",
      "   (Prob: 0.0002)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "def generate_sequence_and_evaluate(model, tokenizer, gene_id, actual_sequence, max_length=100, top_k=5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Prepare the initial input tokens\n",
    "        input_tokens = tokenizer.encode(f'{gene_id}[SOS]')\n",
    "        input_tokens = torch.tensor(input_tokens, dtype=torch.long).unsqueeze(0)\n",
    "        \n",
    "        x = input_tokens\n",
    "        # Process the input through the model's layers\n",
    "        x = model.transformer.wte(x)  # Word token embeddings\n",
    "        pos = torch.arange(0, x.size(1), dtype=torch.long, device=x.device)\n",
    "        x = x + model.transformer.wpe(pos)  # Positional embeddings\n",
    "        \n",
    "        for block in model.transformer.h:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = model.transformer.ln_f(x)\n",
    "        logits = model.lm_head(x)  # Logits from the final output layer\n",
    "\n",
    "        generated_sequence = []\n",
    "        top_5_predictions_each_step = []\n",
    "        for idx in range(x.size(1)):\n",
    "            logits_slice = logits[:, idx, :]\n",
    "            probs = softmax(logits_slice, dim=-1)\n",
    "            top_probs, top_indices = torch.topk(probs, top_k, dim=1)\n",
    "            top_tokens = [tokenizer.decode([idx]) for idx in top_indices[0].tolist()]  # Decode each token id to its corresponding token\n",
    "\n",
    "            if tokenizer.token_to_id['[EOS]'] in top_indices[0]:\n",
    "                break  # Stop if EOS token is among top predictions\n",
    "            \n",
    "            generated_sequence.append(top_indices[0][0].item())  # Append the most probable token to the sequence\n",
    "            top_5_predictions_each_step.append((top_probs[0].tolist(), top_tokens))  # Store probabilities and tokens\n",
    "\n",
    "        # Decode the generated sequence to a string using the tokenizer\n",
    "        predicted_sequence = tokenizer.decode(generated_sequence)\n",
    "\n",
    "    # Compute Levenshtein distance and accuracy\n",
    "    levenshtein_dist = levenshtein_distance(predicted_sequence, actual_sequence)\n",
    "    accuracy = 1 - (levenshtein_dist / max(len(predicted_sequence), len(actual_sequence)))\n",
    "\n",
    "    return {\n",
    "        'gene_id': gene_id,\n",
    "        'predicted_sequence': predicted_sequence,\n",
    "        'actual_sequence': actual_sequence,\n",
    "        'levenshtein_distance': levenshtein_dist,\n",
    "        'accuracy': accuracy,\n",
    "        'top_5_predictions_each_step': top_5_predictions_each_step  # Include top 5 predictions for each step in the output\n",
    "    }\n",
    "\n",
    "# Example usage, provide the actual_sequence that corresponds to the gene_id\n",
    "results = generate_sequence_and_evaluate(model, tokenizer, \"ENSG000101148584\", \"GGATACGATACATGGA\", top_k=5)\n",
    "print(\"Generated Sequence:\", results['predicted_sequence'])\n",
    "print(\"Actual Sequence:\", results['actual_sequence'])\n",
    "print(\"Levenshtein Distance:\", results['levenshtein_distance'])\n",
    "print(\"Accuracy:\", results['accuracy'])\n",
    "for step, (probs, tokens) in enumerate(results['top_5_predictions_each_step']):\n",
    "    print(f\"Step {step}:\")\n",
    "    for prob, token in zip(probs, tokens):\n",
    "        print(f\"  {token} (Prob: {prob:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
